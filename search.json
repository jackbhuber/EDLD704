[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDLD704: Methods and Instruments for Data Collection",
    "section": "",
    "text": "The purpose of this course is to teach you about research methods and instruments for collecting data. The course will expose you to several research methods and offer guidance for collecting both quantitative and qualitative data. You should finish the course knowing:\n\nhow to select an appropriate research method for investigating a question for a study arising from a problem of practice\nhow to conceptualize, develop, and test an instrument for collecting data\nhow to evaluate the quality of data collected, and how to express your evidence-based argument in clear, simple prose, using APA format, to a skeptical reader\n\nTo provide you opportunity to learn these skills, the course expects you to complete two projects: one quantitative, one qualitative. In each project, you will:\n\nframe one or more questions to guide inquiry into a problem of practice\nselect a research method appropriate to the nature and purpose of your inquiry question/s,\nconceptualize and design a small study using your selected research method,\ndesign or select instrumentation to collect data,\ncollect a small sample of data,\ncritically evaluate the quality of your data\ndraw any appropriate inferences or make any appropriate claims from your study."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Questions, Methods, and Data",
    "section": "",
    "text": "In your setting, surely you’ve seen data collected and presented in various ways to get your attention, stimulate your thinking, illuminate an issue, and the like. Maybe you’ve done such data-related work yourself.\nLike appreciation for fine art or food acquired over time, you probably have a sense of “bad” data when you see it.\nDo you also have a sense of “good” data when you see it? And can it still be “good” even if you disagree with it?\nI want this course to help you collect better data from now on by focusing your attention on how data are collected – that is, methods – for conducting inquiry, which includes considerations how to collect data in ways that optimize their quality.\nThe central concerns of this first module are the question and decision of what method to use to carry out a study. As I see it, methods depend, fundamentally, on the nature of the research question: What specific kind of information or understanding does the question seek? Here I make a fundamental and admittedly over-simplistic distinction between qualities and quantities.\nTo investigate questions that betray interest in quantities – “To what extent…?”, “How prevalent…?”, “What predicts….?” – one should use quantitative methods. This includes surveys, experiments, quasi-experiments, and the like.\nTo investigate questions that betray interest in qualities, we employ qualitative designs and instruments. These include interviews, observations, in-depth case studies, analysis of content, and so on.\nOften the research question is so self-evident that the choice of method and instrumentation is obvious.\nBut perhaps just as often we are interested in both, and the qualities-quantities distinction is not so clear cut. To illustrate, consider this case study:"
  },
  {
    "objectID": "intro.html#a-case-study",
    "href": "intro.html#a-case-study",
    "title": "Questions, Methods, and Data",
    "section": "A Case Study",
    "text": "A Case Study\nIn 2006, all public high schools in Washington State were required to administer the state’s large-scale assessment, the Washington Assessments of Student Learning (WASL) in mathematics and reading, to all tenth grade students. By law, these students were the first graduating class required to pass the test in order to receive their high school diplomas. High stakes accountability testing was getting “real.”\nPublic educators throughout the state were anxious. Questions abounded:\n\nHow many students would meet the standard? How close are we?\nWhich students are less likely to reach the standard?\nWhat reforms, interventions, or other restructuring are necessary in elementary and middle school to better prepare students for the high school proficiency standard?\nWhat exactly are the high school proficiency standards in reading and math?\nWhat reforms, interventions, or other restructuring are necessary in high school to better prepare students who failed the test in tenth grade to pass the test by their senior year?\n\nThis state policy was controversial. Many people decried the requirements as fundamentally unfair. Leading psychometricians (experts in test design) criticized the high stakes policies as invalid uses of (largely high quality) standardized tests. Some public educators retired early or found other jobs. Others defended the policies as necessary to bring about long overdue reforms. Many of my colleagues in the district where I worked understandably grumbled and complained … then we all got to work to help our students succeed.\nAt the time, I was somewhere in the middle. It was early in my career in public education. I was employed as a data analyst in my district’s curriculum and instruction department, and on the side I was working on my doctorate. My job, in essence, was to help educators understand student achievement data. I was unique because I had come to education not from classroom teaching but the academic world: sociology. I saw things differently, which is to say I was less partisan and more curious than others. I cared more what data said than what people with sway claimed. Here are the kinds of questions I asked at the time:\n\nWhat is the historical and/or social scientific evidence that these high stakes accountability testing policies actually work? Where and when have these policies already worked?\nAnd what does “actually work” really mean: To improve instruction? To help students overcome demographic disadvantages? \nPart of the theory of action of these accountability policies is “measurement-driven instruction”: testing data should provide instructionally valuable feedback. Teachers should look at data; and when they do, they should see and do …. what?\n“Data-based decisionmaking” is all the rage. But what exactly does it mean for a district or school to be “data-driven”?\nHow does a district or school become “data-driven”? By what process of evolution?\nHigh school teachers already see state assessment at a high level in the summer during inservice days. How often do they look at the state assessment data for their own students? And when they do, how much instructional utility do they derive from the data?\nThe policies assume that external accountability pressure will cause teachers to look at data. Can I test that empirically? Do teachers who perceive more pressure tend to look at state assessment data more often than teachers who perceive less pressure?\nProfessional learning communities are all the rage. They are hailed as a very effective model for organizing and motivating teachers to collaborate. Are high school teachers in professional learning communities more likely to use high school assessment data to improve instruction than those not in professional learning communities?\n\nThey are not the most sophisticated research questions, but they were on a par with what people were writing at the time and they lended themselves readily to data. Now let’s consider all the different kinds of research questions in this topic."
  },
  {
    "objectID": "intro.html#questions-for-quantitative-data",
    "href": "intro.html#questions-for-quantitative-data",
    "title": "Questions, Methods, and Data",
    "section": "Questions for quantitative data",
    "text": "Questions for quantitative data\nThe quantitative questions are fairly obvious:\nHow often do high school teachers use state assessment data? This is a question is a no-brainer because it is about frequency, which ranges from less frequent (“hardly ever’) to more frequent (”all the time”). To study this I needed a sample of teachers who varied in their use of data along this range of frequency.\nAre teachers who perceive more external accountability pressure to improve test scores more likely to examine their own students’ state assessment data more often? This too is an unmistakably quantitative research question (or hypothesis). Implied is comparison between two groups (which is “more likely”) along scales of intensity for accountability pressure (less to more intense), frequency (“rarely” to “often”) of data use. To study this I needed a sample of teachers who varied in their perceptions of accountability pressure and their frequency of data use.\nNotice that questions for quantitative data come from an understanding of the situation of enough sophistication to know what the important variables are and how the variables might be related (do the values of one depend on the values of the other). In most cases, quantitative analysis is deductive; we know what to look for and we understand the situation well enough to test competing theories or understandings.\nQuantitative methods are also appropriate when you want to make generalizations about a population. They seek to show what is generally true of a large number of “cases” (most often, people)."
  },
  {
    "objectID": "intro.html#questions-for-qualitative-data",
    "href": "intro.html#questions-for-qualitative-data",
    "title": "Questions, Methods, and Data",
    "section": "Questions for qualitative data",
    "text": "Questions for qualitative data\nNotice the questions that are more clearly about qualities than quantities. \nWhat does it mean for a district or school to be “data-driven”? Nothing here is quantified or quantifiable. The quest is for attributes or states of “data-driven”. The result could be a typology of different kinds of “data-driven”-ness. Or it could be some kind of evolutionary process with beginning and more advanced stages of development.\nWhat does it mean for a teacher to “use” state assessment data? “Using state assessment data” could mean different things among high schools than my understanding from the district office, the professional research literature, and my background in social science. I needed to talk to sample of teachers to ask them to describe in their own words how they use data.\nWhat sense do high school teachers make of state assessment data? Similar to the question above, I needed to ask teachers to describe what (if anything) they learn from state assessment data in their own words.\nFor each of these questions, the focus is full understanding of a small number of cases (most often, people). Generalization to a large population is NOT the point of qualitative methods. Qualitative methods aim to understand what is deeply true of a small number of cases."
  },
  {
    "objectID": "intro.html#your-turn",
    "href": "intro.html#your-turn",
    "title": "Questions, Methods, and Data",
    "section": "Your turn",
    "text": "Your turn\nHaving considered the different angles for research in this case study, now think about your own problem of practice as it seems to you in your setting or milieu. Maybe this is your nascent capstone project.\nWrite down your guiding question/s that best capture your true interest.\nThen consider the words you’ve used.\nAre you looking to explore something that is not well understood? Do your questions seek understanding of kinds, ways in which, processes, stages, distinctions, classes, forms, and the like? Are these things you can average? (No?) Do you seek understanding of the mental models, theories, understandings, and the like, of how someone in your setting of interest perceives something, or understands what they’re doing? Do you want their own words? Are you interested in the “theory of action” behind a program or organization? Are you interested in identities and self-understandings? Are people’s own metaphors interesting to you? Do you want to deal primarily with “words” data? If yes, then you may be primarily interested in qualitative methods.\nDo you have a good enough understanding of your topic that you know what the important factors or variables are? Is one variable more important than another? Do you want a sense of scope, estimate, size, frequency, magnitude, intensity, extent, prevalence, risk, predictability, regularity, or relationship? Do you want to deal with primarily with “numbers” and “scale” data? If yes, then you may be primarily interested in quantitative methods.\nA final word, for now, about mixed methods:\nThere are good reasons to use mixed methods. You may want to collect some qualitative data (from interviews, observations) from a few cases to more deeply understand something. With better firsthand understanding you can then develop more accurate survey items, frame more relevant questions and hypotheses, and test competing explanations of something.\nMy doctoral dissertation was de facto mixed methods. It began with qualitative work. From my role in the central office I knew a lot about my topic from a global perspective and from the professional literature, but I did not understand teacher work life very deeply. Interviewing a small sample of them helped me better understand my topic from their perspective. But I didn’t stop there; I wanted to make generalizations to a population of teachers. Based on this more sophisticated understanding I was able to frame smarter research questions and better survey items and to specify and estimate more grounded statistical models. My quantitative dissertation study proper owes its quality to the preliminary qualitative work that informed it.\nMixed methods are possible, and may appeal to you philosophically: “Why choose between the two if I can do both? Wouldn’t mixed methods make the most sense and do the most justice to the topic?” True enough. And what more appropriate laboratory for learning different research methods than your doctoral program? But to do any research method well is to negotiate a learning curve, and your time and energy are limited in this fast-paced doctoral program. Do factor that into your discernment of methods. Whatever you decide, I will help you as best I can."
  },
  {
    "objectID": "surveys.html",
    "href": "surveys.html",
    "title": "Surveys",
    "section": "",
    "text": "Required reading:"
  },
  {
    "objectID": "surveys.html#why-do-a-survey",
    "href": "surveys.html#why-do-a-survey",
    "title": "Surveys",
    "section": "Why do a survey?",
    "text": "Why do a survey?\nA survey is an efficient way to collect a large quantity of data on a large number of people in a relatively short amount of time. Then one can use these data to:\n\n“Explore a topic that has not been previously examined” (Burkholder et al. (2020), p. 163)\n“Explain a relationship between two or more variables of interest” (Burkholder et al. (2020), p. 163)\n“Describe the characteristics or attributes of a population” (Burkholder et al. (2020), p. 163)\nMake generalizations about a population of people\nGet a sense of the scope, extent, magnitude, or prevalence of something\nMeasure a construct, such as psychological well-being\n\nDepending on your guiding questions, a survey may be the appropriate method to collect the data you need for your capstone. And at some point in your time in education, you may want or need to conduct a survey. Now is a great time to gain understanding and skill."
  },
  {
    "objectID": "surveys.html#some-key-terms",
    "href": "surveys.html#some-key-terms",
    "title": "Surveys",
    "section": "Some key terms",
    "text": "Some key terms\nSurvey methods have their own vocabulary. Following is a list of key terms you should know when reading about and use when undertaking surveys:\n\n\n\n\n\n\n\nSurvey\nthe “method of collecting data from and about people” (Fink, 2009, quoted in Burkholder et al. (2020), p. 161)\n\n\nSurvey instrument\n“the tool used to gather data–this term is typically used to differentiate the tool from the survey research it supports” (Burkholder et al. (2020), p. 161)\n\n\nQuestionnaire\n“a survey instrument that contains items that the respondent is expected to read and then report his or her own answers” (Burkholder et al. (2020), p. 161)\n\n\nForm\nThe body of the survey or test instrument where all of the items are assembled. A survey may use two different forms, such as Form A and Form B, each of which contains the same items in different orders, to examine the effects of item order on responses Item a question on a survey or test that gathers responses from a respondent and creates variation\n\n\nResponse categories\nCategories, such as those found on a Likert scale (1=Strongly Agree, 2=Agree, etc.), that a respondent may use to respond to a survey item\n\n\nDescriptor\nA descriptive label, such as “Strongly Agree”, that one applies to a response category to make it the response meaningful to the respondent\n\n\nRespondent\nan individual who responds to an item and/or survey instrument\n\n\nPilot\na phase of the survey project when an investigator uses instrument to collect sample data for the purpose of improving the instrument and/or data collection procedures\n\n\nOperational\nhe final phase of the survey project in which the instrument collects data of sufficient quality to collect “real” data for the purpose of supporting high stakes decisions"
  },
  {
    "objectID": "surveys.html#properties-of-a-poor-quality-survey",
    "href": "surveys.html#properties-of-a-poor-quality-survey",
    "title": "Surveys",
    "section": "Properties of a poor quality survey",
    "text": "Properties of a poor quality survey\nWe’ve all seen and/or taken poor quality surveys. Here are a few characteristics of poor quality surveys:\n\nThe items are too long. The survey writer is wordy and/or has too much “voice.” It’s difficult to tell what the respondent is thinking and/or what the respondent is responding to.\nThe items lead the respondent. The items are trying to “educate” or push the respondent toward something. The survey has an agenda.\nThe items and/or response categories are limited in scope, and thus they exclude some respondents. A good example is the “Neutral/No Opinion” category.\nThe survey is too long. By the end of the instrument, respondents will tire and stop responding to items.\nThe survey uses so many open-ended items that it is collecting primarily qualitative data and is essentially an interview project. It will yield a wealth of comments, many of which say very similar things, and may be laborious to read and code.\n\nPlease consider using these as litmus tests for the quality of your own future survey work."
  },
  {
    "objectID": "surveys.html#how-to-design-a-high-quality-survey",
    "href": "surveys.html#how-to-design-a-high-quality-survey",
    "title": "Surveys",
    "section": "How to design a high quality survey",
    "text": "How to design a high quality survey\nUse these steps, selected from the literature and my own professional experience doing dozens of surveys over the years, to design a high quality survey:\n\n1. Clarify the purpose of your survey.\nBegin by considering why choose a survey instead of another method to answer your question. Why is a survey appropriate for your question?\nWhat is the time frame for your survey? Will it be a timely, issue-specific “fact-finding” survey that reveals “How many people think X?” about a specific issue (such as a curriculum adoption, or a bond election)? Will the survey lose its relevance after the moment has passed? Or does your survey aim to measure something ongoing in the culture (like a school climate survey) and thus be used multiple times to build trend data?\nWill the data be used to quantify the magnitude of sentiment, attitude, opinion, or behavior? Will the data be used to describe a population? Will the data be used to compare groups on a sentiment, attitude, opinion, or behavior? Or could your data be used to explain which variables are stronger predictors of an outcome than others?\n\n\n2. Draft a map of the survey.\nDesigning a good survey is much like designing a good student achievement test. The starting point for a student achievement test is not test questions, it is a map of the different learning objectives. The same goes with a survey. A survey project should begin with a high level list of the overall questions one wants answered.\n\n\n3. Sample carefully.\nWhat is the sampling method? Is it a convenience sample of people available? If so, what are some sources of sampling bias? What relevant respondents might be left out? What profit might you gain from select a probability sample?\n\n\n4. Use validated items from other established survey instruments, or write your own high quality items.\nLearn from the experts, when possible:\n\nWriting Survey Questions (The Pew Research Center)\nBest Practices (Washington State University)\n\nKeep survey items short and simple. Avoid long, wordy items that could confuse the respondent.\nAvoid double-barreled items. Keep survey items focused on one dimension at a time. (I saw this in education over and over and over again.)\nDon’t lead or force data from the respondent. Example: Many times I have heard people intentionally withhold a “Neutral/No Opinion” category in order to “force” the respondent to take a stand on an issue. I don’t like that practice. If a respondent truly does not understand or have an opinion about a topic, I would rather know that than force the respondent to yield an artificial (and, in my mind, invalid) response.\nAllow response categories that span the range of all possible responses. Response categories on a survey item should be exhaustive and mutually exclusive. This assumes you know the full range of possible responses. If you don’t, consider asking this item first as an open-ended item on a pilot survey. Then you can ask it as a closed item on your operational survey.\nBe judicious in your use of open-ended items. Allowing respondents to respond in their own words will create a large volume of comments that will take time to read, and many of the comments say similar things. Use open-ended items on a pilot instrument when you don’t fully understand an issue and want to see the full range of possible types of responses to it. These types of responses can then become response categories on a closed item on an operational version of the survey.\n\n\n5. Pilot the questionnaire before going live.\nShow the questionnaire to a small sample of intended respondents. Ask them to take the survey, noting the following:\nConfusion. Is the purpose of the survey clear to the respondent? Is any part of it confusing to the respondent in any way? Are any items confusing as worded?\nBias. Does the survey truly capture the full scope of respondent experience on the issue? Are some options left out? Do some items lead or force the respondent?\nLength. Is the survey an appropriate length? Does the survey tire out respondent? Aim for no longer than 15 minutes.\nValidity. Does the survey capture the thinking, (mis)conceptions, ideas, beliefs, sentiments, attitudes, opinions, and/or behaviors you designed it to capture? Or does it also capture extraneous information? Use a “think aloud” method of asking the respondent to verbalize their responses as they take the survey.\nThere is not always time to pilot a survey. But in my experience, piloting has always improved the quality of my surveys.\n\n\n\n\n\nBurkholder, G. J., K. A. Cox, L. M. Crawford, and Hitchcock. 2020. Research Design and Methods: An Applied Guide for the Scholar-Practitioner. Sage."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Burkholder, G. J., K. A. Cox, L. M. Crawford, and Hitchcock. 2020.\nResearch Design and Methods: An Applied Guide for the\nScholar-Practitioner. Sage."
  }
]